{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# @title 1. Load Fixed Train/Test Data and Group by Traffic\n",
        "\n",
        "# CELL 1 [TAG: parameters]\n",
        "# ---------------------------------------------------------\n",
        "# Default parameters (Airflow will OVERWRITE these)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Input: Fixed CSV files from MinIO\n",
        "# Expected columns: speed_kmh, acceleration_ms2 (or acc_forward), segment_id (optional)\n",
        "INPUT_TRAIN_CSV = \"s3://models-quality-eval/data/train.csv\"\n",
        "INPUT_TEST_CSV = \"s3://models-quality-eval/data/test.csv\"\n",
        "\n",
        "# Output: Grouped segments (same format as before)\n",
        "RUN_TIMESTAMP = \"2025-01-01_00-00-00\"  # Injected by Airflow\n",
        "OUTPUT_TRAIN_DATA = \"s3://models-quality-eval/2025-01-01_00-00-00/train/grouped_segments.pkl\"\n",
        "OUTPUT_TEST_DATA = \"s3://models-quality-eval/2025-01-01_00-00-00/test/grouped_segments.pkl\"\n",
        "\n",
        "# Traffic classification parameters\n",
        "SPEED_THRESHOLD = 25.0     # km/h (below = heavy traffic, above = light traffic)\n",
        "MIN_DURATION_TRAIN = 5     # seconds (minimum segment length for training data)\n",
        "MIN_DURATION_TEST = 0      # seconds (minimum segment length for test data - accept all)\n",
        "\n",
        "# MinIO Credentials\n",
        "MINIO_ENDPOINT = \"http://localhost:9000\"\n",
        "MINIO_ACCESS_KEY = \"admin\"\n",
        "MINIO_SECRET_KEY = \"password123\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import s3fs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: MinIO Configuration\n",
        "fs = s3fs.S3FileSystem(\n",
        "    key=MINIO_ACCESS_KEY,\n",
        "    secret=MINIO_SECRET_KEY,\n",
        "    client_kwargs={'endpoint_url': MINIO_ENDPOINT}\n",
        ")\n",
        "\n",
        "print(f\"✅ Connected to MinIO at {MINIO_ENDPOINT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Helper Function - Group by Traffic\n",
        "def group_by_traffic(csv_path, speed_threshold, min_duration):\n",
        "    \"\"\"\n",
        "    Load CSV and group segments by traffic condition.\n",
        "    \n",
        "    Args:\n",
        "        csv_path: S3 path to CSV file\n",
        "        speed_threshold: Speed threshold (km/h) for heavy vs light traffic\n",
        "        min_duration: Minimum segment length in seconds\n",
        "    \n",
        "    Returns:\n",
        "        List of [heavy_traffic_segments, light_traffic_segments]\n",
        "        Each segment is a numpy array of shape (n, 2) with columns [speed_kmh, accel_ms2]\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading: {csv_path}\")\n",
        "    \n",
        "    # Load CSV from MinIO\n",
        "    with fs.open(csv_path, 'rb') as f:\n",
        "        df = pd.read_csv(f)\n",
        "    \n",
        "    print(f\"  Total rows: {len(df)}\")\n",
        "    print(f\"  Columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Validate required columns (flexible acceleration column naming)\n",
        "    if 'speed_kmh' not in df.columns:\n",
        "        raise ValueError(\"Missing required column: speed_kmh\")\n",
        "    \n",
        "    # Handle both acceleration column naming conventions\n",
        "    if 'acceleration_ms2' in df.columns:\n",
        "        accel_col = 'acceleration_ms2'\n",
        "        print(f\"  Using acceleration column: acceleration_ms2\")\n",
        "    elif 'acc_forward' in df.columns:\n",
        "        accel_col = 'acc_forward'\n",
        "        print(f\"  Using acceleration column: acc_forward\")\n",
        "    else:\n",
        "        raise ValueError(\"Missing acceleration column: need either 'acceleration_ms2' or 'acc_forward'\")\n",
        "    \n",
        "    # Initialize grouping\n",
        "    grouped_segments = [[], []]  # Index 0: Heavy Traffic, Index 1: Light Traffic\n",
        "    \n",
        "    # If there's a segment_id column, group by it; otherwise treat as one segment\n",
        "    if 'segment_id' in df.columns:\n",
        "        # Group by segment_id and identify contiguous blocks\n",
        "        df = df.sort_values('segment_id')\n",
        "        df['block_id'] = (df['segment_id'] != df['segment_id'].shift()).cumsum()\n",
        "        segments_iter = df.groupby('block_id')\n",
        "    else:\n",
        "        # Treat entire dataset as segments based on continuity\n",
        "        # Create blocks based on time gaps or just split into chunks\n",
        "        print(\"  No segment_id found - treating as continuous data\")\n",
        "        # Simple approach: create one large segment\n",
        "        segments_iter = [(0, df)]\n",
        "    \n",
        "    segment_count = 0\n",
        "    heavy_count = 0\n",
        "    light_count = 0\n",
        "    \n",
        "    for _, segment_df in segments_iter:\n",
        "        # Filter by minimum duration\n",
        "        if len(segment_df) < min_duration:\n",
        "            continue\n",
        "        \n",
        "        # Extract speed and acceleration\n",
        "        speed_kmh = segment_df['speed_kmh'].values\n",
        "        accel_ms2 = segment_df[accel_col].values\n",
        "        \n",
        "        # Create segment array [speed, accel]\n",
        "        segment_array = np.column_stack([speed_kmh, accel_ms2])\n",
        "        \n",
        "        # Classify by average speed\n",
        "        avg_speed = np.mean(speed_kmh)\n",
        "        \n",
        "        if avg_speed < speed_threshold:\n",
        "            # Heavy Traffic (low speed)\n",
        "            grouped_segments[0].append(segment_array)\n",
        "            heavy_count += 1\n",
        "        else:\n",
        "            # Light Traffic (high speed)\n",
        "            grouped_segments[1].append(segment_array)\n",
        "            light_count += 1\n",
        "        \n",
        "        segment_count += 1\n",
        "    \n",
        "    print(f\"  ✅ Processed {segment_count} segments:\")\n",
        "    print(f\"     - Heavy Traffic: {heavy_count} segments\")\n",
        "    print(f\"     - Light Traffic: {light_count} segments\")\n",
        "    \n",
        "    return grouped_segments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Load and Group Train Data\n",
        "print(\"=\"*60)\n",
        "print(\"PROCESSING TRAIN DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_grouped = group_by_traffic(\n",
        "    csv_path=INPUT_TRAIN_CSV,\n",
        "    speed_threshold=SPEED_THRESHOLD,\n",
        "    min_duration=MIN_DURATION_TRAIN\n",
        ")\n",
        "\n",
        "# Save train data\n",
        "with fs.open(OUTPUT_TRAIN_DATA, 'wb') as f:\n",
        "    pickle.dump(train_grouped, f)\n",
        "\n",
        "print(f\"\\n✅ Train data saved to: {OUTPUT_TRAIN_DATA}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Load and Group Test Data\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROCESSING TEST DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_grouped = group_by_traffic(\n",
        "    csv_path=INPUT_TEST_CSV,\n",
        "    speed_threshold=SPEED_THRESHOLD,\n",
        "    min_duration=MIN_DURATION_TEST\n",
        ")\n",
        "\n",
        "# Save test data\n",
        "with fs.open(OUTPUT_TEST_DATA, 'wb') as f:\n",
        "    pickle.dump(test_grouped, f)\n",
        "\n",
        "print(f\"\\n✅ Test data saved to: {OUTPUT_TEST_DATA}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Run Timestamp: {RUN_TIMESTAMP}\")\n",
        "print(f\"Speed Threshold: {SPEED_THRESHOLD} km/h\")\n",
        "print(f\"Min Duration (Train): {MIN_DURATION_TRAIN} seconds\")\n",
        "print(f\"Min Duration (Test): {MIN_DURATION_TEST} seconds (accepts all segments)\")\n",
        "print(f\"\\nTrain Set:\")\n",
        "print(f\"  - Heavy Traffic: {len(train_grouped[0])} segments\")\n",
        "print(f\"  - Light Traffic: {len(train_grouped[1])} segments\")\n",
        "print(f\"  - Total: {len(train_grouped[0]) + len(train_grouped[1])} segments\")\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  - Heavy Traffic: {len(test_grouped[0])} segments\")\n",
        "print(f\"  - Light Traffic: {len(test_grouped[1])} segments\")\n",
        "print(f\"  - Total: {len(test_grouped[0]) + len(test_grouped[1])} segments\")\n",
        "print(f\"\\n✅ Pipeline ready for training!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
