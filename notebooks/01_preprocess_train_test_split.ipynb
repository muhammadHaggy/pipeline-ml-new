{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# @title 1. Group Segments with Train/Test Split (80:20)\n",
        "\n",
        "# CELL 1 [TAG: parameters]\n",
        "\n",
        "# Default parameters (Airflow will inject these)\n",
        "\n",
        "INPUT_FOLDER = \"s3://processed-data/\"       # Output from Step 0\n",
        "\n",
        "RUN_TIMESTAMP = \"2025-01-01_00-00-00\"  # Injected by Airflow\n",
        "OUTPUT_TRAIN_DATA = \"s3://models-quality-eval/2025-01-01_00-00-00/train/grouped_segments.pkl\"\n",
        "OUTPUT_TEST_DATA = \"s3://models-quality-eval/2025-01-01_00-00-00/test/grouped_segments.pkl\"\n",
        "\n",
        "SPEED_THRESHOLD = 25.0  # km/h\n",
        "MIN_DURATION = 15       # seconds\n",
        "TRAIN_RATIO = 0.8       # 80% train, 20% test\n",
        "RANDOM_SEED = 42        # For reproducibility\n",
        "\n",
        "MINIO_ENDPOINT = \"http://localhost:9000\" \n",
        "MINIO_ACCESS_KEY = \"admin\"\n",
        "MINIO_SECRET_KEY = \"password123\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: Imports\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "\n",
        "import s3fs\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: MinIO Configuration\n",
        "\n",
        "# Define connection settings for S3-compatible storage\n",
        "\n",
        "MINIO_ENDPOINT = os.environ.get(\"MINIO_ENDPOINT\", \"http://minio:9000\")\n",
        "\n",
        "MINIO_KEY = os.environ.get(\"MINIO_ACCESS_KEY\", \"admin\")\n",
        "\n",
        "MINIO_SECRET = os.environ.get(\"MINIO_SECRET_KEY\", \"password123\")\n",
        "\n",
        "# Initialize S3 Filesystem\n",
        "\n",
        "fs = s3fs.S3FileSystem(\n",
        "    key=MINIO_ACCESS_KEY,      # Uses the variable injected by Papermill\n",
        "    secret=MINIO_SECRET_KEY,   # Uses the variable injected by Papermill\n",
        "    client_kwargs={'endpoint_url': MINIO_ENDPOINT}\n",
        ")\n",
        "\n",
        "# Pandas storage options\n",
        "storage_options = {\n",
        "    \"key\": MINIO_ACCESS_KEY,\n",
        "    \"secret\": MINIO_SECRET_KEY,\n",
        "    \"client_kwargs\": {\"endpoint_url\": MINIO_ENDPOINT}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Logic\n",
        "\n",
        "def preprocess_and_group(folder):\n",
        "\n",
        "    grouped_segments = [[], []] # Index 0: Heavy, Index 1: Light\n",
        "\n",
        "    # 1. List all processed 1Hz files in the S3 bucket\n",
        "\n",
        "    # fs.glob returns paths like 'bucket/file.csv', we need to ensure 's3://' prefix if needed\n",
        "\n",
        "    # or just pass the path directly to fs.open\n",
        "\n",
        "    try:\n",
        "\n",
        "        file_paths = fs.glob(f\"{folder.replace('s3://', '')}*.csv\")\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"Error listing files in {folder}: {e}\")\n",
        "\n",
        "        return grouped_segments\n",
        "\n",
        "    print(f\"Found {len(file_paths)} processed trips.\")\n",
        "\n",
        "    for file_path in file_paths:\n",
        "\n",
        "        print(f\"Processing {file_path}...\")\n",
        "\n",
        "        \n",
        "\n",
        "        # Read directly from S3\n",
        "\n",
        "        with fs.open(file_path, 'rb') as f:\n",
        "\n",
        "            df = pd.read_csv(f)\n",
        "\n",
        "        \n",
        "\n",
        "        # Ensure sorted by time\n",
        "\n",
        "        df = df.sort_values('seconds_elapsed')\n",
        "\n",
        "        # Identify contiguous blocks of the same segment_id\n",
        "\n",
        "        # (Step 0 already ensured segment_id is populated via Google Roads API)\n",
        "\n",
        "        df['block_id'] = (df['segment_id'] != df['segment_id'].shift()).cumsum()\n",
        "\n",
        "        for _, block in df.groupby('block_id'):\n",
        "\n",
        "            # Filter out short noise segments\n",
        "\n",
        "            if len(block) < MIN_DURATION: \n",
        "\n",
        "                continue\n",
        "\n",
        "            # Extract Physics\n",
        "\n",
        "            speed_kmh = block['speed_kmh'].values\n",
        "\n",
        "            \n",
        "\n",
        "            # CRITICAL CHANGE: Use the Sensor-Fused Acceleration from Step 0\n",
        "\n",
        "            # Instead of recalculating it via derivative (np.diff), we use the \n",
        "\n",
        "            # high-precision 'acc_forward' calculated from the IMU.\n",
        "\n",
        "            if 'acc_forward' in block.columns:\n",
        "\n",
        "                accel_ms2 = block['acc_forward'].values\n",
        "\n",
        "            else:\n",
        "\n",
        "                # Fallback if column missing (shouldn't happen with Step 0)\n",
        "\n",
        "                accel_ms2 = np.zeros(len(speed_kmh))\n",
        "\n",
        "                accel_ms2[:-1] = np.diff(speed_kmh) / 3.6\n",
        "\n",
        "            \n",
        "\n",
        "            # Stack into matrix: [Speed, Acceleration]\n",
        "\n",
        "            segment_data = np.column_stack((speed_kmh, accel_ms2))\n",
        "\n",
        "            # Classify based on Average Speed\n",
        "\n",
        "            if np.mean(speed_kmh) <= SPEED_THRESHOLD:\n",
        "\n",
        "                grouped_segments[0].append(segment_data) # Heavy\n",
        "\n",
        "            else:\n",
        "\n",
        "                grouped_segments[1].append(segment_data) # Light\n",
        "\n",
        "    return grouped_segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Execution & Train/Test Split\n",
        "\n",
        "print(\"=== Preprocessing and Grouping ===\")\n",
        "data = preprocess_and_group(INPUT_FOLDER)\n",
        "\n",
        "print(f\"Result: {len(data[0])} Heavy segments, {len(data[1])} Light segments.\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Split each group into train/test\n",
        "train_data = [[], []]\n",
        "test_data = [[], []]\n",
        "\n",
        "for group_idx in range(2):\n",
        "    group_name = \"Heavy\" if group_idx == 0 else \"Light\"\n",
        "    segments = data[group_idx]\n",
        "    \n",
        "    if len(segments) == 0:\n",
        "        print(f\"Warning: No segments in {group_name} traffic group.\")\n",
        "        continue\n",
        "    \n",
        "    # Calculate split index\n",
        "    n_total = len(segments)\n",
        "    n_train = int(n_total * TRAIN_RATIO)\n",
        "    \n",
        "    # Shuffle indices\n",
        "    indices = np.random.permutation(n_total)\n",
        "    train_indices = indices[:n_train]\n",
        "    test_indices = indices[n_train:]\n",
        "    \n",
        "    # Split segments\n",
        "    train_data[group_idx] = [segments[i] for i in train_indices]\n",
        "    test_data[group_idx] = [segments[i] for i in test_indices]\n",
        "    \n",
        "    print(f\"{group_name} Traffic: {len(train_data[group_idx])} train, {len(test_data[group_idx])} test segments\")\n",
        "\n",
        "print(f\"\\n=== Split Summary ===\")\n",
        "print(f\"Train Ratio: {TRAIN_RATIO * 100}%\")\n",
        "print(f\"Random Seed: {RANDOM_SEED}\")\n",
        "print(f\"Run Timestamp: {RUN_TIMESTAMP}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Save Train and Test Data\n",
        "\n",
        "# Save Training Data\n",
        "print(f\"\\nSaving training data to {OUTPUT_TRAIN_DATA}...\")\n",
        "with fs.open(OUTPUT_TRAIN_DATA, 'wb') as f:\n",
        "    pickle.dump(train_data, f)\n",
        "\n",
        "# Save Test Data\n",
        "print(f\"Saving test data to {OUTPUT_TEST_DATA}...\")\n",
        "with fs.open(OUTPUT_TEST_DATA, 'wb') as f:\n",
        "    pickle.dump(test_data, f)\n",
        "\n",
        "print(\"âœ… Done. Train and test datasets saved successfully.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
