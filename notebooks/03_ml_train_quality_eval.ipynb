{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Quality Eval: Train Model (Simplified)\n",
        "Train on train set only - matches your SpeedAccelerationPredictor approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# CELL 1: Parameters\n",
        "RUN_TIMESTAMP = \"2025-01-01_00-00-00\"\n",
        "INPUT_TRAIN_DATA = \"s3://models-quality-eval-ml/train/train_data.pkl\"\n",
        "OUTPUT_ML_MODEL_PATH = \"s3://models-quality-eval-ml/models/speed_accel_model.pkl\"\n",
        "\n",
        "# Use cross-validation for extra robustness? (slower but more stable)\n",
        "USE_CROSS_VALIDATION = False  # Set True if you have limited data\n",
        "\n",
        "MINIO_ENDPOINT = \"http://minio:9000\"\n",
        "MINIO_ACCESS_KEY = \"admin\"\n",
        "MINIO_SECRET_KEY = \"password123\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import s3fs\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAS_XGB = True\n",
        "except:\n",
        "    HAS_XGB = False\n",
        "    print(\"‚ö†Ô∏è  XGBoost not available, will skip\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: MinIO Config\n",
        "fs = s3fs.S3FileSystem(\n",
        "    key=MINIO_ACCESS_KEY,\n",
        "    secret=MINIO_SECRET_KEY,\n",
        "    client_kwargs={'endpoint_url': MINIO_ENDPOINT}\n",
        ")\n",
        "\n",
        "storage_options = {\n",
        "    \"key\": MINIO_ACCESS_KEY,\n",
        "    \"secret\": MINIO_SECRET_KEY,\n",
        "    \"client_kwargs\": {\"endpoint_url\": MINIO_ENDPOINT}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Load Training Data\n",
        "print(f\"=== ML Quality Eval: Training on Train Set ===\")\n",
        "print(f\"Run Timestamp: {RUN_TIMESTAMP}\")\n",
        "print(f\"\\nLoading training data from {INPUT_TRAIN_DATA}...\")\n",
        "\n",
        "try:\n",
        "    with fs.open(INPUT_TRAIN_DATA, 'rb') as f:\n",
        "        df = pickle.load(f)\n",
        "    \n",
        "    if isinstance(df, pd.DataFrame):\n",
        "        print(f\"‚úÖ Loaded DataFrame with {len(df):,} rows\")\n",
        "    else:\n",
        "        raise TypeError(f\"Expected DataFrame, got {type(df)}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: {INPUT_TRAIN_DATA} not found. Run step 01 first.\")\n",
        "    raise\n",
        "\n",
        "print(f\"Training dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns[:15])}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Column Normalization\n",
        "print(\"\\nNormalizing column names...\")\n",
        "\n",
        "column_mapping = {\n",
        "    'timestamp_sensor': 'timestamp',\n",
        "    'latitude': 'position_lat',\n",
        "    'longitude': 'position_long',\n",
        "    'speed_ms': 'speed_mps',\n",
        "    'altitude': 'enhanced_altitude',\n",
        "    'acc_forward': 'acceleration_m_s2',\n",
        "}\n",
        "\n",
        "for old, new in column_mapping.items():\n",
        "    if old in df.columns and new not in df.columns:\n",
        "        df.rename(columns={old: new}, inplace=True)\n",
        "\n",
        "print(\"‚úÖ Column normalization complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Advanced Feature Engineering & Cleaning (Training Set)\n",
        "print(\"\\nPerforming Advanced Feature Engineering & Cleaning...\")\n",
        "\n",
        "# 1. SORT DATA (Wajib)\n",
        "if 'trip_id' in df.columns:\n",
        "    df = df.sort_values(['trip_id', 'seconds_elapsed'])\n",
        "else:\n",
        "    df = df.sort_values('seconds_elapsed')\n",
        "\n",
        "# 2. REPAIR DATA (Jika speed rusak/0 semua)\n",
        "if df['speed_mps'].mean() < 0.1 and 'position_lat' in df.columns:\n",
        "    print(\"‚ö†Ô∏è Speed data broken. Recalculating from GPS coords...\")\n",
        "    # Simple Euclidean distance approx\n",
        "    df['d_lat'] = df.groupby('trip_id')['position_lat'].diff().fillna(0)\n",
        "    df['d_lon'] = df.groupby('trip_id')['position_long'].diff().fillna(0)\n",
        "    df['calc_dist'] = np.sqrt(df['d_lat']**2 + df['d_lon']**2) * 111000 \n",
        "    df['speed_mps'] = df['calc_dist'].rolling(window=3, center=True).mean().fillna(0)\n",
        "\n",
        "# 3. FEATURE ENGINEERING\n",
        "# A. Basic Lag Features\n",
        "grouper = df.groupby('trip_id')['speed_mps'] if 'trip_id' in df.columns else df['speed_mps']\n",
        "df['speed_mps_prev1'] = grouper.shift(1).fillna(0)\n",
        "df['speed_mps_prev2'] = grouper.shift(2).fillna(0)\n",
        "df['accel_prev1'] = (df['speed_mps_prev1'] - df['speed_mps_prev2']).fillna(0)\n",
        "\n",
        "# B. Rolling Features (Trend 5 detik) - AGAR GRAFIK TIDAK PATAH-PATAH\n",
        "# Shift(1) dulu baru rolling agar tidak bocor\n",
        "roll_src = grouper.shift(1)\n",
        "df['speed_roll_mean_5s'] = roll_src.rolling(5, min_periods=1).mean().fillna(0)\n",
        "df['speed_roll_std_5s'] = roll_src.rolling(5, min_periods=1).std().fillna(0)\n",
        "\n",
        "# C. Segment Context (Agar model tau ini jalan ngebut/pelan)\n",
        "if 'segment_id' in df.columns:\n",
        "    df['segment_avg_speed'] = df.groupby('segment_id')['speed_mps'].transform('mean')\n",
        "else:\n",
        "    # Fallback: Expanding mean\n",
        "    if 'trip_id' in df.columns:\n",
        "        df['segment_avg_speed'] = df.groupby('trip_id')['speed_mps'].expanding().mean().reset_index(0, drop=True)\n",
        "    else:\n",
        "        df['segment_avg_speed'] = df['speed_mps'].expanding().mean()\n",
        "\n",
        "# D. Map Features\n",
        "if 'enhanced_altitude' in df.columns:\n",
        "    grouper_alt = df.groupby('trip_id')['enhanced_altitude'] if 'trip_id' in df.columns else df['enhanced_altitude']\n",
        "    df['elev_gain_m'] = grouper_alt.diff().fillna(0)\n",
        "else:\n",
        "    df['elev_gain_m'] = 0\n",
        "\n",
        "if 'label_traffic' in df.columns:\n",
        "    traffic_map = {'heavy': 2, 'moderate': 1, 'light': 0}\n",
        "    df['traffic_level'] = df['label_traffic'].map(traffic_map).fillna(1)\n",
        "else:\n",
        "    df['traffic_level'] = 1 \n",
        "\n",
        "if 'position_lat' in df.columns:\n",
        "    if 'trip_id' in df.columns:\n",
        "        df['delta_lat'] = df.groupby('trip_id')['position_lat'].diff().fillna(0)\n",
        "        df['delta_lon'] = df.groupby('trip_id')['position_long'].diff().fillna(0)\n",
        "    else:\n",
        "        df['delta_lat'] = df['position_lat'].diff().fillna(0)\n",
        "        df['delta_lon'] = df['position_long'].diff().fillna(0)\n",
        "    df['delta_dist'] = np.sqrt(df['delta_lat']**2 + df['delta_lon']**2)\n",
        "else:\n",
        "    df['delta_lat']=0; df['delta_lon']=0; df['delta_dist']=0\n",
        "\n",
        "if 'bearing' in df.columns:\n",
        "    grouper_bear = df.groupby('trip_id')['bearing'] if 'trip_id' in df.columns else df['bearing']\n",
        "    df['heading_change'] = grouper_bear.diff().fillna(0)\n",
        "    df['turn_count'] = (np.abs(df['heading_change']) > 15).astype(int)\n",
        "else:\n",
        "    df['heading_change']=0; df['turn_count']=0\n",
        "\n",
        "df = df.fillna(0)\n",
        "\n",
        "# 4. FILTER STATIONARY (PENTING AGAR TIDAK FLATLINE)\n",
        "print(f\"Original Rows: {len(df)}\")\n",
        "df = df[df['speed_mps'] > 0.5] # Buang data parkir\n",
        "print(f\"‚úÖ Filtered (Moving) Rows: {len(df)}\")\n",
        "\n",
        "print(\"‚úÖ Advanced Feature Engineering Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6.5: DATA SANITY CHECK & REPAIR (Wajib!)\n",
        "print(\"\\n=== CHECKING & REPAIRING SPEED DATA ===\")\n",
        "\n",
        "# 1. Cek apakah speed isinya 0 semua\n",
        "max_speed = df['speed_mps'].max()\n",
        "mean_speed = df['speed_mps'].mean()\n",
        "print(f\"Original Data -> Max Speed: {max_speed:.4f} m/s, Mean Speed: {mean_speed:.4f} m/s\")\n",
        "\n",
        "# 2. JIKA DATA RUSAK (0 SEMUA), HITUNG ULANG DARI JARAK (Geopy)\n",
        "if mean_speed < 0.1: # Ambang batas kecurigaan\n",
        "    print(\"‚ö†Ô∏è WARNING: Speed data seems broken (too low/zero). Recalculating from GPS...\")\n",
        "    \n",
        "    # Pastikan delta_dist sudah dihitung di Cell 6\n",
        "    # Speed = Jarak / Waktu (Asumsi data 1Hz, jadi dt=1)\n",
        "    # Kita pakai moving average biar ga noise\n",
        "    df['speed_mps'] = df['delta_dist'].rolling(window=3, center=True).mean().fillna(0)\n",
        "    \n",
        "    # Recalculate Accel\n",
        "    df['accel_from_speed'] = df['speed_mps'].diff().fillna(0)\n",
        "    \n",
        "    print(f\"‚úÖ REPAIRED Data -> Max Speed: {df['speed_mps'].max():.4f} m/s\")\n",
        "    \n",
        "# 3. Drop data diam (Optional: Biar model fokus belajar jalan)\n",
        "# Hapus baris yang speed-nya 0 (parkir)\n",
        "initial_len = len(df)\n",
        "df = df[df['speed_mps'] > 0.5] # Ambil yang bergerak > 0.5 m/s\n",
        "print(f\"Dropped {initial_len - len(df)} stationary rows. New training size: {len(df)}\")\n",
        "\n",
        "if len(df) == 0:\n",
        "    raise ValueError(\"‚ùå STOP: Tidak ada data bergerak di Training Set! Cek Step 01 (Split).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: Prepare Training Data\n",
        "feature_cols = [\n",
        "    'speed_mps_prev1', 'speed_mps_prev2', 'accel_prev1',\n",
        "    'speed_roll_mean_5s', 'speed_roll_std_5s', # NEW\n",
        "    'segment_avg_speed',                       # NEW\n",
        "    'enhanced_altitude', 'bearing', \n",
        "    'delta_dist', 'delta_lat', 'delta_lon',\n",
        "    'elev_gain_m', 'traffic_level', \n",
        "    'heading_change', 'turn_count'\n",
        "]\n",
        "\n",
        "# Ensure cols exist\n",
        "for c in feature_cols:\n",
        "    if c not in df.columns: df[c] = 0\n",
        "\n",
        "X = df[feature_cols].values\n",
        "y_speed = df['speed_mps'].values\n",
        "\n",
        "print(f\"‚úÖ Features: {feature_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: Scale Features\n",
        "print(\"\\nScaling features...\")\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"‚úÖ Features scaled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9: Train-Val Split (RAW FEATURES)\n",
        "X_train, X_val, y_speed_train, y_speed_val = train_test_split(\n",
        "    X, y_speed, test_size=0.2, random_state=42, shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"\\nTrain/Validation split:\")\n",
        "print(f\"   Training samples: {len(X_train):,}\")\n",
        "print(f\"   Validation samples: {len(X_val):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9.5: Scale ONLY for SVR & ANN\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 10: Define 5 Core Models (Environment Compatible)\n",
        "print(\"Defining 5 core models (XGBoost using MAE objective)...\")\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "models = {\n",
        "    # 1. RANDOM FOREST (Baseline)\n",
        "    'RandomForest': RandomForestRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=30,        # Deep trees to capture variance\n",
        "        min_samples_leaf=2,  \n",
        "        min_samples_split=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "    # 2. DECISION TREE (Simple)\n",
        "    'DecisionTree': DecisionTreeRegressor(\n",
        "        max_depth=20,\n",
        "        min_samples_leaf=4,\n",
        "        random_state=42\n",
        "    ),\n",
        "\n",
        "    # 3. SVR (Support Vector)\n",
        "    'SVR': SVR(\n",
        "        C=100,           # High regularization C for aggressive fitting\n",
        "        gamma=0.1,       \n",
        "        epsilon=0.01,    \n",
        "        kernel='rbf'\n",
        "    ),\n",
        "\n",
        "    # 4. ANN (Neural Network)\n",
        "    'ANN': MLPRegressor(\n",
        "        hidden_layer_sizes=(128, 64, 32), \n",
        "        activation='relu',\n",
        "        alpha=0.0001,\n",
        "        learning_rate_init=0.001,\n",
        "        max_iter=500,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# 5. XGBOOST (Compatible & Robust)\n",
        "if HAS_XGB:\n",
        "    models['XGBoost'] = xgb.XGBRegressor(\n",
        "        objective='reg:absoluteerror',\n",
        "        n_estimators=600,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=8,            \n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=1.0,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\"‚úÖ {len(models)} models defined (including XGBoost MAE)\")\n",
        "else:\n",
        "    print(f\"‚úÖ {len(models)} models defined (XGBoost skipped)\")\n",
        "\n",
        "for name in models.keys():\n",
        "    print(f\"   - {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 11: Training Loop (Smart Scaling)\n",
        "print(\"\\n=== STARTING TRAINING LOOP ===\")\n",
        "\n",
        "results = []\n",
        "trained_models = {}\n",
        "\n",
        "# Pastikan target 1D array\n",
        "y_train_flat = y_speed_train.ravel()\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nüöÄ Training: {name}\")\n",
        "    \n",
        "    # --- LOGIKA SCALING (CRITICAL) ---\n",
        "    # SVR dan ANN (Neural Network) SANGAT butuh data scaled (-1 s/d 1)\n",
        "    # Tree-based (RF, DT, XGB) seringkali lebih bagus pakai data asli (biar tau angka real)\n",
        "    if name in ['SVR', 'ANN']:\n",
        "        X_tr = X_train_scaled\n",
        "        X_v  = X_val_scaled\n",
        "        print(\"   -> Using SCALED data (StandardScaler)\")\n",
        "    else:\n",
        "        X_tr = X_train\n",
        "        X_v  = X_val\n",
        "        print(\"   -> Using RAW data (Original Units)\")\n",
        "    \n",
        "    # Train\n",
        "    model.fit(X_tr, y_train_flat)\n",
        "    \n",
        "    # Predict (Validation)\n",
        "    y_pred = model.predict(X_v)\n",
        "    \n",
        "    # Metrics\n",
        "    r2 = r2_score(y_speed_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_speed_val, y_pred))\n",
        "    mae = mean_absolute_error(y_speed_val, y_pred)\n",
        "    \n",
        "    # Simpan Hasil\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'R¬≤': r2,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae\n",
        "    })\n",
        "    trained_models[name] = model\n",
        "    \n",
        "    print(f\"   R¬≤   : {r2:.4f}\")\n",
        "    print(f\"   RMSE : {rmse:.4f} m/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 12: Select Best Model (by RMSE, like your original)\n",
        "df_results = pd.DataFrame(results).sort_values('RMSE').reset_index(drop=True)\n",
        "\n",
        "\n",
        "best_name = df_results.iloc[0]['Model']\n",
        "best_model = trained_models[best_name]\n",
        "best_r2 = df_results.iloc[0]['R¬≤']\n",
        "best_rmse = df_results.iloc[0]['RMSE']\n",
        "best_mae = df_results.iloc[0]['MAE']\n",
        "\n",
        "print(\"\\n‚úÖ BEST MODEL (Speed):\", best_name)\n",
        "print(df_results)\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL SELECTED\")\n",
        "print(f\"‚û° Model : {best_name}\")\n",
        "print(f\"‚û° R¬≤    : {best_r2:.4f}\")\n",
        "print(f\"‚û° RMSE  : {best_rmse:.4f} m/s ({best_rmse*3.6:.2f} km/h)\")\n",
        "print(f\"‚û° MAE   : {best_mae:.4f} m/s ({best_mae*3.6:.2f} km/h)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model = models[best_name]\n",
        "\n",
        "if best_name in ['SVR', 'ANN']:\n",
        "    X_full = scaler.fit_transform(X)\n",
        "else:\n",
        "    X_full = X\n",
        "\n",
        "final_model.fit(X_full, y_speed)\n",
        "\n",
        "print(\"‚úÖ Final model trained on full data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 13.5: Calculate & Validate Derived Acceleration\n",
        "# ---------------------------------------------------\n",
        "# Logic: Accel_Pred(t) = Speed_Pred(t) - Speed_Pred(t-1)\n",
        "# Kita hitung accel MURNI dari prediksi speed, tanpa mengintip data asli.\n",
        "\n",
        "print(\"\\n=== DERIVED ACCELERATION VALIDATION ===\")\n",
        "\n",
        "# 1. Generate Prediksi Speed di Data Validasi\n",
        "# Pastikan X_val urut waktu (shuffle=False saat split di step sebelumnya sangat PENTING!)\n",
        "y_pred_speed = final_model.predict(X_val_scaled if best_name in ['SVR', 'ANN'] else X_val)\n",
        "\n",
        "# 2. Hitung Predicted Acceleration (Tanpa Data Leakage)\n",
        "# Menggunakan pandas diff() untuk menghitung selisih t dengan t-1\n",
        "pred_speed_series = pd.Series(y_pred_speed)\n",
        "pred_accel = pred_speed_series.diff().fillna(0).values\n",
        "\n",
        "# 3. Hitung Real Acceleration (Sebagai Kunci Jawaban)\n",
        "# Kita hitung dari y_speed_val asli agar apple-to-apple perbandingannya\n",
        "real_speed_series = pd.Series(y_speed_val)\n",
        "real_accel = real_speed_series.diff().fillna(0).values\n",
        "\n",
        "# 4. Hitung Metrik Akurasi Akselerasi\n",
        "accel_rmse = np.sqrt(mean_squared_error(real_accel, pred_accel))\n",
        "accel_mae = mean_absolute_error(real_accel, pred_accel)\n",
        "accel_r2 = r2_score(real_accel, pred_accel)\n",
        "\n",
        "print(f\"üìä Derived Acceleration Metrics (Calculated from Predicted Speed):\")\n",
        "print(f\"   R¬≤ Score : {accel_r2:.4f}\")\n",
        "print(f\"   RMSE     : {accel_rmse:.4f} m/s¬≤\")\n",
        "print(f\"   MAE      : {accel_mae:.4f} m/s¬≤\")\n",
        "\n",
        "# 5. Visualisasi Cepat (Optional)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(real_accel[:200], label='Real Accel (Diff from Real Speed)', alpha=0.7)\n",
        "plt.plot(pred_accel[:200], label='Pred Accel (Diff from Pred Speed)', alpha=0.7)\n",
        "plt.title(\"Derived Acceleration: Real vs Predicted (First 200 pts)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Saving training logs to CSV...\")\n",
        "df['predicted_speed'] = final_model.predict(scaler.transform(df[feature_cols].values))\n",
        "df.to_csv(\"train_predictions_log.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 14: Save Model (YOUR ARTIFACT FORMAT)\n",
        "print(f\"\\nSaving model to {OUTPUT_ML_MODEL_PATH}...\")\n",
        "\n",
        "# Match YOUR artifact structure exactly\n",
        "artifact = {\n",
        "    \"scaler\": scaler,\n",
        "    \"speed_model\": final_model,\n",
        "    \"speed_model_name\": best_name,\n",
        "    \"feature_cols\": feature_cols,\n",
        "    \"train_metrics\": {\n",
        "        \"r2\": float(best_r2),\n",
        "        \"rmse\": float(best_rmse),\n",
        "        \"mae\": float(best_mae),\n",
        "        \"model_name\": best_name\n",
        "    }\n",
        "}\n",
        "\n",
        "with fs.open(OUTPUT_ML_MODEL_PATH, 'wb') as f:\n",
        "    pickle.dump(artifact, f)\n",
        "\n",
        "print(\"‚úÖ ML Model saved successfully\")\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ TRAINING COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Run Timestamp: {RUN_TIMESTAMP}\")\n",
        "print(f\"Best Model: {best_name}\")\n",
        "print(f\"Training R¬≤: {best_r2:.4f}\")\n",
        "print(f\"Training RMSE: {best_rmse:.4f} m/s ({best_rmse*3.6:.2f} km/h)\")\n",
        "print(f\"Training MAE: {best_mae:.4f} m/s ({best_mae*3.6:.2f} km/h)\")\n",
        "print(f\"Model saved to: {OUTPUT_ML_MODEL_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
