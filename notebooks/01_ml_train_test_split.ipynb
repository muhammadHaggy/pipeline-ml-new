{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Quality Eval: Train/Test Split (80:20)\n",
        "Handles both multi-file and single-file scenarios intelligently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "RUN_TIMESTAMP = \"2025-01-01_00-00-00\"\n",
        "INPUT_FOLDER = \"s3://processed-data\"\n",
        "OUTPUT_TRAIN_DATA = \"s3://models-quality-eval-ml/train/train_data.pkl\"\n",
        "OUTPUT_TEST_DATA = \"s3://models-quality-eval-ml/test/test_data.pkl\"\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "RANDOM_SEED = 42\n",
        "MIN_ROWS_FOR_SPLIT = 1000\n",
        "\n",
        "MINIO_ENDPOINT = \"http://minio:9000\"\n",
        "MINIO_ACCESS_KEY = \"admin\"\n",
        "MINIO_SECRET_KEY = \"password123\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import s3fs\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== ML Quality Eval: Train/Test Split ===\")\n",
        "print(f\"Run Timestamp: {RUN_TIMESTAMP}\")\n",
        "print(f\"MinIO Endpoint: {MINIO_ENDPOINT}\")\n",
        "print(f\"Input Folder: {INPUT_FOLDER}\")\n",
        "\n",
        "fs = s3fs.S3FileSystem(\n",
        "    key=MINIO_ACCESS_KEY,\n",
        "    secret=MINIO_SECRET_KEY,\n",
        "    client_kwargs={'endpoint_url': MINIO_ENDPOINT}\n",
        ")\n",
        "\n",
        "storage_options = {\n",
        "    \"key\": MINIO_ACCESS_KEY,\n",
        "    \"secret\": MINIO_SECRET_KEY,\n",
        "    \"client_kwargs\": {\"endpoint_url\": MINIO_ENDPOINT}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bucket if not exists\n",
        "try:\n",
        "    fs.ls('models-quality-eval-ml')\n",
        "    print(\"‚úÖ Bucket 'models-quality-eval-ml' exists\")\n",
        "except FileNotFoundError:\n",
        "    fs.mkdir('models-quality-eval-ml')\n",
        "    print(\"‚úÖ Created bucket 'models-quality-eval-ml'\")\n",
        "\n",
        "# Create folder structure\n",
        "folders = [\n",
        "    'models-quality-eval-ml/train',\n",
        "    'models-quality-eval-ml/test',\n",
        "    'models-quality-eval-ml/models',\n",
        "    'models-quality-eval-ml/metrics'\n",
        "]\n",
        "\n",
        "for folder in folders:\n",
        "    try:\n",
        "        fs.ls(folder)\n",
        "    except FileNotFoundError:\n",
        "        fs.mkdir(folder)\n",
        "        print(f\"Created folder: {folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_prefix = INPUT_FOLDER.replace('s3://', '')\n",
        "if not input_prefix.endswith('/'):\n",
        "    input_prefix += '/'\n",
        "\n",
        "try:\n",
        "    raw_paths = fs.glob(f\"{input_prefix}*.csv\")\n",
        "    file_paths = [f\"s3://{p}\" for p in raw_paths]\n",
        "    \n",
        "    print(f\"Found {len(file_paths)} CSV file(s)\")\n",
        "    for i, path in enumerate(file_paths, 1):\n",
        "        print(f\"   {i}. {path.split('/')[-1]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error listing files in {INPUT_FOLDER}: {e}\")\n",
        "    raise\n",
        "\n",
        "if len(file_paths) == 0:\n",
        "    raise ValueError(f\"No CSV files found in {INPUT_FOLDER}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Determine Split Strategy\n",
        "if len(file_paths) >= 10:\n",
        "    split_strategy = \"file-level\"\n",
        "    print(f\"   Multiple files available ({len(file_paths)} files)\")\n",
        "elif len(file_paths) >= 2:\n",
        "    split_strategy = \"file-level\"\n",
        "    print(f\"   Limited files: {len(file_paths)} files\")\n",
        "    print(f\"   Warning: Split may not be fully representative\")\n",
        "else:\n",
        "    split_strategy = \"row-level\"\n",
        "    print(f\"   Single file detected: {file_paths[0].split('/')[-1]}\")\n",
        "    print(f\"   Will split by trip_id or random rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: Perform Split with Auto-Generated trip_id\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Helper function untuk extract nama file jadi trip_id\n",
        "def get_trip_id_from_path(path):\n",
        "    # Ambil nama file paling belakang, buang .csv\n",
        "    filename = path.split('/')[-1]\n",
        "    return filename.replace('.csv', '').replace('.CSV', '')\n",
        "\n",
        "if split_strategy == \"file-level\":\n",
        "    # === STRATEGY 1: BANYAK FILE (Split File-nya) ===\n",
        "    \n",
        "    train_files, test_files = train_test_split(\n",
        "        file_paths,\n",
        "        train_size=TRAIN_RATIO,\n",
        "        random_state=RANDOM_SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n=== Split Results ===\")\n",
        "    print(f\"Train Files: {len(train_files)} ({len(train_files)/len(file_paths)*100:.1f}%)\")\n",
        "    print(f\"Test Files: {len(test_files)} ({len(test_files)/len(file_paths)*100:.1f}%)\")\n",
        "    \n",
        "    # --- LOAD TRAIN FILES ---\n",
        "    train_dfs = []\n",
        "    for f in train_files:\n",
        "        try:\n",
        "            print(f\"  ‚Üí Loading Train: {f.split('/')[-1]}\")\n",
        "            df_tmp = pd.read_csv(f, storage_options=storage_options)\n",
        "            \n",
        "            # [BARU] Generate trip_id dari nama file jika belum ada\n",
        "            if 'trip_id' not in df_tmp.columns:\n",
        "                df_tmp['trip_id'] = get_trip_id_from_path(f)\n",
        "                \n",
        "            train_dfs.append(df_tmp)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è  Error loading {f}: {e}\")\n",
        "    \n",
        "    if len(train_dfs) == 0:\n",
        "        raise ValueError(\"Could not load any training files\")\n",
        "    \n",
        "    train_data = pd.concat(train_dfs, ignore_index=True)\n",
        "    print(f\"‚úÖ Train data combined: {train_data.shape}\")\n",
        "    \n",
        "    # --- LOAD TEST FILES ---\n",
        "    test_dfs = []\n",
        "    for f in test_files:\n",
        "        try:\n",
        "            print(f\"  ‚Üí Loading Test: {f.split('/')[-1]}\")\n",
        "            df_tmp = pd.read_csv(f, storage_options=storage_options)\n",
        "            \n",
        "            # [BARU] Generate trip_id dari nama file jika belum ada\n",
        "            if 'trip_id' not in df_tmp.columns:\n",
        "                df_tmp['trip_id'] = get_trip_id_from_path(f)\n",
        "                \n",
        "            test_dfs.append(df_tmp)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è  Error loading {f}: {e}\")\n",
        "    \n",
        "    if len(test_dfs) == 0:\n",
        "        raise ValueError(\"Could not load any test files\")\n",
        "    \n",
        "    test_data = pd.concat(test_dfs, ignore_index=True)\n",
        "    print(f\"‚úÖ Test data combined: {test_data.shape}\")\n",
        "    \n",
        "else:\n",
        "    # === STRATEGY 2: SATU FILE (Split Baris-nya) ===\n",
        "    print(f\"\\nLoading single file: {file_paths[0].split('/')[-1]}\")\n",
        "    df_full = pd.read_csv(file_paths[0], storage_options=storage_options)\n",
        "    \n",
        "    # [BARU] Generate trip_id dari nama file jika belum ada\n",
        "    if 'trip_id' not in df_full.columns:\n",
        "        print(\"   Generating trip_id from filename...\")\n",
        "        df_full['trip_id'] = get_trip_id_from_path(file_paths[0])\n",
        "        \n",
        "    print(f\"‚úÖ Total rows: {len(df_full):,}\")\n",
        "    \n",
        "    # Check minimum rows\n",
        "    if len(df_full) < MIN_ROWS_FOR_SPLIT:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: {len(df_full):,} rows < MIN_ROWS_FOR_SPLIT ({MIN_ROWS_FOR_SPLIT:,})\")\n",
        "        print(f\"   Metrics may be unstable with limited data\")\n",
        "    \n",
        "    # Split logic\n",
        "    if len(df_full['trip_id'].unique()) > 1:\n",
        "        # Jika dalam satu file ternyata ada banyak trip_id\n",
        "        print(f\"\\n‚úÖ Splitting by existing trip_id...\")\n",
        "        unique_trips = df_full['trip_id'].unique()\n",
        "        \n",
        "        train_trips, test_trips = train_test_split(\n",
        "            unique_trips,\n",
        "            train_size=TRAIN_RATIO,\n",
        "            random_state=RANDOM_SEED,\n",
        "            shuffle=True\n",
        "        )\n",
        "        \n",
        "        train_data = df_full[df_full['trip_id'].isin(train_trips)].copy()\n",
        "        test_data = df_full[df_full['trip_id'].isin(test_trips)].copy()\n",
        "    else:\n",
        "        # Jika benar-benar cuma 1 trip (single file, single trip)\n",
        "        print(f\"\\n‚úÖ Single trip detected. Splitting rows randomly (Time-series split recommended but using random for simplicity)\")\n",
        "        \n",
        "        train_data, test_data = train_test_split(\n",
        "            df_full,\n",
        "            train_size=TRAIN_RATIO,\n",
        "            random_state=RANDOM_SEED,\n",
        "            shuffle=False # Shuffle False supaya urutan waktu terjaga (opsional, tapi bagus buat time series)\n",
        "        )\n",
        "    \n",
        "    print(f\"\\n=== Split Results === \")\n",
        "    print(f\"Train rows: {len(train_data):,} ({len(train_data)/len(df_full)*100:.1f}%)\")\n",
        "    print(f\"Test rows: {len(test_data):,} ({len(test_data)/len(df_full)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Split complete (seed={RANDOM_SEED})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: Verify Data Integrity\n",
        "print(f\"\\n=== Data Verification ===\")\n",
        "print(f\"Train shape: {train_data.shape}\")\n",
        "print(f\"Test shape: {test_data.shape}\")\n",
        "print(f\"Total samples: {len(train_data) + len(test_data):,}\")\n",
        "\n",
        "# Show columns\n",
        "print(f\"\\nColumns ({len(train_data.columns)}):\")\n",
        "print(f\"  First 10: {list(train_data.columns[:10])}\")\n",
        "if len(train_data.columns) > 10:\n",
        "    print(f\"  ... and {len(train_data.columns) - 10} more\")\n",
        "\n",
        "# Check for trip_id overlap (data leakage)\n",
        "if 'trip_id' in train_data.columns and split_strategy == 'row-level':\n",
        "    train_trips = set(train_data['trip_id'].unique())\n",
        "    test_trips = set(test_data['trip_id'].unique())\n",
        "    overlap = train_trips.intersection(test_trips)\n",
        "    \n",
        "    if len(overlap) > 0:\n",
        "        print(f\"\\nWARNING: {len(overlap)} trips in both train and test!\")\n",
        "        print(f\"   This indicates data leakage\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ No trip_id overlap (no data leakage)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9: Save Train and Test Data\n",
        "print(f\"\\n=== Saving Results ===\")\n",
        "\n",
        "print(f\"Saving train data to {OUTPUT_TRAIN_DATA}...\")\n",
        "with fs.open(OUTPUT_TRAIN_DATA, 'wb') as f:\n",
        "    pickle.dump(train_data, f)\n",
        "print(f\"‚úÖ Train data saved\")\n",
        "\n",
        "print(f\"\\nSaving test data to {OUTPUT_TEST_DATA}...\")\n",
        "with fs.open(OUTPUT_TEST_DATA, 'wb') as f:\n",
        "    pickle.dump(test_data, f)\n",
        "print(f\"‚úÖ Test data saved\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ TRAIN/TEST SPLIT COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Strategy: {split_strategy.upper()}\")\n",
        "print(f\"Train samples: {len(train_data):,}\")\n",
        "print(f\"Test samples: {len(test_data):,}\")\n",
        "print(f\"Split ratio: {TRAIN_RATIO*100:.0f}% / {(1-TRAIN_RATIO)*100:.0f}%\")\n",
        "print(f\"Random seed: {RANDOM_SEED}\")\n",
        "print(f\"\\nOutput files:\")\n",
        "print(f\"  Train: {OUTPUT_TRAIN_DATA}\")\n",
        "print(f\"  Test: {OUTPUT_TEST_DATA}\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
