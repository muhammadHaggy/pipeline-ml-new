{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# @title 1. Group Segments (Heavy vs Light)\n",
        "\n",
        "# CELL 1 [TAG: parameters]\n",
        "\n",
        "# Default parameters (Airflow will inject these)\n",
        "\n",
        "INPUT_FOLDER = \"s3://processed-data/\"       # Output from Step 0\n",
        "\n",
        "OUTPUT_GROUPED_DATA = \"s3://models/grouped_segments.pkl\"\n",
        "SPEED_THRESHOLD = 25.0  # km/h\n",
        "MIN_DURATION = 15       # seconds\n",
        "\n",
        "MINIO_ENDPOINT = \"http://localhost:9000\" \n",
        "MINIO_ACCESS_KEY = \"admin\"\n",
        "MINIO_SECRET_KEY = \"password123\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: Imports\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "\n",
        "import s3fs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: MinIO Configuration\n",
        "\n",
        "# Define connection settings for S3-compatible storage\n",
        "\n",
        "MINIO_ENDPOINT = os.environ.get(\"MINIO_ENDPOINT\", \"http://minio:9000\")\n",
        "\n",
        "MINIO_KEY = os.environ.get(\"MINIO_ACCESS_KEY\", \"admin\")\n",
        "\n",
        "MINIO_SECRET = os.environ.get(\"MINIO_SECRET_KEY\", \"password123\")\n",
        "\n",
        "# Initialize S3 Filesystem\n",
        "\n",
        "fs = s3fs.S3FileSystem(\n",
        "    key=MINIO_ACCESS_KEY,      # Uses the variable injected by Papermill\n",
        "    secret=MINIO_SECRET_KEY,   # Uses the variable injected by Papermill\n",
        "    client_kwargs={'endpoint_url': MINIO_ENDPOINT}\n",
        ")\n",
        "\n",
        "# Pandas storage options\n",
        "storage_options = {\n",
        "    \"key\": MINIO_ACCESS_KEY,\n",
        "    \"secret\": MINIO_SECRET_KEY,\n",
        "    \"client_kwargs\": {\"endpoint_url\": MINIO_ENDPOINT}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Logic\n",
        "\n",
        "def preprocess_and_group(folder):\n",
        "\n",
        "    grouped_segments = [[], []] # Index 0: Heavy, Index 1: Light\n",
        "\n",
        "    # 1. List all processed 1Hz files in the S3 bucket\n",
        "\n",
        "    # fs.glob returns paths like 'bucket/file.csv', we need to ensure 's3://' prefix if needed\n",
        "\n",
        "    # or just pass the path directly to fs.open\n",
        "\n",
        "    try:\n",
        "\n",
        "        file_paths = fs.glob(f\"{folder.replace('s3://', '')}*.csv\")\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"Error listing files in {folder}: {e}\")\n",
        "\n",
        "        return grouped_segments\n",
        "\n",
        "    print(f\"Found {len(file_paths)} processed trips.\")\n",
        "\n",
        "    for file_path in file_paths:\n",
        "\n",
        "        print(f\"Processing {file_path}...\")\n",
        "\n",
        "        \n",
        "\n",
        "        # Read directly from S3\n",
        "\n",
        "        with fs.open(file_path, 'rb') as f:\n",
        "\n",
        "            df = pd.read_csv(f)\n",
        "\n",
        "        \n",
        "\n",
        "        # Ensure sorted by time\n",
        "\n",
        "        df = df.sort_values('seconds_elapsed')\n",
        "\n",
        "        # Identify contiguous blocks of the same segment_id\n",
        "\n",
        "        # (Step 0 already ensured segment_id is populated via Google Roads API)\n",
        "\n",
        "        df['block_id'] = (df['segment_id'] != df['segment_id'].shift()).cumsum()\n",
        "\n",
        "        for _, block in df.groupby('block_id'):\n",
        "\n",
        "            # Filter out short noise segments\n",
        "\n",
        "            if len(block) < MIN_DURATION: \n",
        "\n",
        "                continue\n",
        "\n",
        "            # Extract Physics\n",
        "\n",
        "            speed_kmh = block['speed_kmh'].values\n",
        "\n",
        "            \n",
        "\n",
        "            # CRITICAL CHANGE: Use the Sensor-Fused Acceleration from Step 0\n",
        "\n",
        "            # Instead of recalculating it via derivative (np.diff), we use the \n",
        "\n",
        "            # high-precision 'acc_forward' calculated from the IMU.\n",
        "\n",
        "            if 'acc_forward' in block.columns:\n",
        "\n",
        "                accel_ms2 = block['acc_forward'].values\n",
        "\n",
        "            else:\n",
        "\n",
        "                # Fallback if column missing (shouldn't happen with Step 0)\n",
        "\n",
        "                accel_ms2 = np.zeros(len(speed_kmh))\n",
        "\n",
        "                accel_ms2[:-1] = np.diff(speed_kmh) / 3.6\n",
        "\n",
        "            \n",
        "\n",
        "            # Stack into matrix: [Speed, Acceleration]\n",
        "\n",
        "            segment_data = np.column_stack((speed_kmh, accel_ms2))\n",
        "\n",
        "            # Classify based on Average Speed\n",
        "\n",
        "            if np.mean(speed_kmh) <= SPEED_THRESHOLD:\n",
        "\n",
        "                grouped_segments[0].append(segment_data) # Heavy\n",
        "\n",
        "            else:\n",
        "\n",
        "                grouped_segments[1].append(segment_data) # Light\n",
        "\n",
        "    return grouped_segments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Execution & Save\n",
        "\n",
        "data = preprocess_and_group(INPUT_FOLDER)\n",
        "\n",
        "print(f\"Result: {len(data[0])} Heavy segments, {len(data[1])} Light segments.\")\n",
        "\n",
        "# Save Pickle to S3\n",
        "\n",
        "print(f\"Saving model data to {OUTPUT_GROUPED_DATA}...\")\n",
        "\n",
        "with fs.open(OUTPUT_GROUPED_DATA, 'wb') as f:\n",
        "\n",
        "    pickle.dump(data, f)\n",
        "\n",
        "print(\"âœ… Done.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
