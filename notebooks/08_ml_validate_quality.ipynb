{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Quality Eval: Validate on Test Set\n",
        "Uses YOUR acceleration calculation: predicted_speed(t) - speed_prev1(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# CELL 1: Parameters\n",
        "RUN_TIMESTAMP = \"2025-01-01_00-00-00\"\n",
        "INPUT_TEST_DATA = \"s3://models-quality-eval-ml/test/test_data.pkl\"\n",
        "INPUT_ML_MODEL_PATH = \"s3://models-quality-eval-ml/models/speed_accel_model.pkl\"\n",
        "OUTPUT_METRICS_PATH = \"s3://models-quality-eval-ml/metrics/quality_metrics.json\"\n",
        "OUTPUT_PLOT_PATH = \"s3://models-quality-eval-ml/metrics/validation_plots.png\"\n",
        "\n",
        "# Quality Thresholds (relaxed for single-file testing)\n",
        "MIN_R2_SCORE = 0.85\n",
        "MAX_SPEED_RMSE = 2.5  # m/s\n",
        "MAX_ACCEL_RMSE = 0.7  # m/s¬≤\n",
        "MAX_SPEED_MAE = 2.0   # m/s\n",
        "MAX_ACCEL_MAE = 0.5   # m/s¬≤\n",
        "\n",
        "MINIO_ENDPOINT = \"http://minio:9000\"\n",
        "MINIO_ACCESS_KEY = \"admin\"\n",
        "MINIO_SECRET_KEY = \"password123\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import io\n",
        "import s3fs\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: MinIO Configuration\n",
        "fs = s3fs.S3FileSystem(\n",
        "    key=MINIO_ACCESS_KEY,\n",
        "    secret=MINIO_SECRET_KEY,\n",
        "    client_kwargs={'endpoint_url': MINIO_ENDPOINT}\n",
        ")\n",
        "\n",
        "storage_options = {\n",
        "    \"key\": MINIO_ACCESS_KEY,\n",
        "    \"secret\": MINIO_SECRET_KEY,\n",
        "    \"client_kwargs\": {\"endpoint_url\": MINIO_ENDPOINT}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Load Test Data and Model\n",
        "print(f\"=== ML Quality Validation ===\")\n",
        "print(f\"Run Timestamp: {RUN_TIMESTAMP}\")\n",
        "print(f\"\\nLoading artifacts...\")\n",
        "\n",
        "# Load test data\n",
        "try:\n",
        "    with fs.open(INPUT_TEST_DATA, 'rb') as f:\n",
        "        df_test = pickle.load(f)\n",
        "    \n",
        "    if isinstance(df_test, pd.DataFrame):\n",
        "        print(f\"‚úÖ Loaded test DataFrame with {len(df_test):,} rows\")\n",
        "    else:\n",
        "        raise TypeError(f\"Expected DataFrame, got {type(df_test)}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: {INPUT_TEST_DATA} not found. Run step 01 first.\")\n",
        "    raise\n",
        "\n",
        "# Load trained model\n",
        "try:\n",
        "    with fs.open(INPUT_ML_MODEL_PATH, 'rb') as f:\n",
        "        model_artifact = pickle.load(f)\n",
        "    \n",
        "    scaler = model_artifact['scaler']\n",
        "    speed_model = model_artifact['speed_model']\n",
        "    feature_cols = model_artifact['feature_cols']\n",
        "    model_name = model_artifact.get('speed_model_name', 'Unknown')\n",
        "    train_metrics = model_artifact.get('train_metrics', {})\n",
        "    \n",
        "    print(f\"‚úÖ Model loaded: {model_name}\")\n",
        "    if train_metrics:\n",
        "        print(f\"   Training R¬≤: {train_metrics.get('r2', 'N/A'):.4f}\")\n",
        "        print(f\"   Training RMSE: {train_metrics.get('rmse', 'N/A'):.4f} m/s\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: {INPUT_ML_MODEL_PATH} not found. Run step 03 first.\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\nTest dataset shape: {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Feature Engineering (Same as Training) - CLEANED\n",
        "print(\"\\nPerforming feature engineering on test set...\")\n",
        "\n",
        "column_mapping = {\n",
        "    'timestamp_sensor': 'timestamp',\n",
        "    'latitude': 'position_lat',\n",
        "    'longitude': 'position_long',\n",
        "    'speed_ms': 'speed_mps',\n",
        "    'altitude': 'enhanced_altitude',\n",
        "    'acc_forward': 'acceleration',\n",
        "}\n",
        "\n",
        "for old, new in column_mapping.items():\n",
        "    if old in df_test.columns and new not in df_test.columns:\n",
        "        df_test.rename(columns={old: new}, inplace=True)\n",
        "\n",
        "\n",
        "if 'trip_id' in df_test.columns:\n",
        "    df_test = df_test.sort_values(['trip_id', 'seconds_elapsed'])\n",
        "else:\n",
        "    df_test = df_test.sort_values('seconds_elapsed')\n",
        "\n",
        "\n",
        "if 'position_lat' in df_test.columns and 'position_long' in df_test.columns:\n",
        "    if 'trip_id' in df_test.columns:\n",
        "        df_test['delta_lat'] = df_test.groupby('trip_id')['position_lat'].diff().fillna(0)\n",
        "        df_test['delta_lon'] = df_test.groupby('trip_id')['position_long'].diff().fillna(0)\n",
        "    else:\n",
        "        df_test['delta_lat'] = df_test['position_lat'].diff().fillna(0)\n",
        "        df_test['delta_lon'] = df_test['position_long'].diff().fillna(0)\n",
        "    df_test['delta_dist'] = np.sqrt(df_test['delta_lat']**2 + df_test['delta_lon']**2)\n",
        "else:\n",
        "    df_test['delta_lat'] = 0\n",
        "    df_test['delta_lon'] = 0\n",
        "    df_test['delta_dist'] = 0\n",
        "\n",
        "\n",
        "if 'enhanced_altitude' in df_test.columns:\n",
        "    if 'trip_id' in df_test.columns:\n",
        "        df_test['elev_gain_m'] = df_test.groupby('trip_id')['enhanced_altitude'].diff().fillna(0)\n",
        "    else:\n",
        "        df_test['elev_gain_m'] = df_test['enhanced_altitude'].diff().fillna(0)\n",
        "else:\n",
        "    df_test['elev_gain_m'] = 0\n",
        "\n",
        "\n",
        "if 'label_traffic' in df_test.columns:\n",
        "    traffic_map = {'heavy': 2, 'moderate': 1, 'light': 0}\n",
        "    df_test['traffic_level'] = df_test['label_traffic'].map(traffic_map).fillna(1)\n",
        "else:\n",
        "    df_test['traffic_level'] = 1\n",
        "\n",
        "\n",
        "if 'bearing' not in df_test.columns:\n",
        "    df_test['bearing'] = 0\n",
        "    \n",
        "if 'trip_id' in df_test.columns:\n",
        "    df_test['heading_change'] = df_test.groupby('trip_id')['bearing'].diff().fillna(0)\n",
        "else:\n",
        "    df_test['heading_change'] = df_test['bearing'].diff().fillna(0)\n",
        "    \n",
        "df_test['turn_count'] = (np.abs(df_test['heading_change']) > 15).astype(int)\n",
        "\n",
        "\n",
        "df_test = df_test.fillna(0)\n",
        "\n",
        "print(\"‚úÖ Feature engineering complete\")\n",
        "print(\"‚ö†Ô∏è  NOTE: speed_mps_prev1/prev2 will be created in autoregressive loop (Cell 7)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Prepare Test Features\n",
        "# Ensure all feature columns exist\n",
        "missing_cols = [col for col in feature_cols if col not in df_test.columns]\n",
        "if missing_cols:\n",
        "    print(f\"‚ö†Ô∏è  Warning: Missing columns {missing_cols}. Creating with zeros.\")\n",
        "    for col in missing_cols:\n",
        "        df_test[col] = 0\n",
        "\n",
        "X_test = df_test[feature_cols].values\n",
        "\n",
        "# Scale features using trained scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\n‚úÖ Test data prepared:\")\n",
        "print(f\"   X_test shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: Make Autoregressive Predictions (Real-World Simulation)\n",
        "\n",
        "print(\"\\n=== GENERATING AUTOREGRESSIVE PREDICTIONS (High Accuracy Mode) ===\")\n",
        "print(\"‚ö†Ô∏è This loops through data row-by-row. It may take a few minutes for large datasets.\")\n",
        "\n",
        "# 1. Sort data is CRITICAL for autoregressive logic\n",
        "if 'trip_id' in df_test.columns:\n",
        "    df_test = df_test.sort_values(['trip_id', 'seconds_elapsed']).reset_index(drop=True)\n",
        "else:\n",
        "    df_test = df_test.sort_values('seconds_elapsed').reset_index(drop=True)\n",
        "\n",
        "# 2. Initialize columns\n",
        "df_test['predicted_speed'] = 0.0\n",
        "\n",
        "idx_prev1 = feature_cols.index('speed_mps_prev1')\n",
        "idx_prev2 = feature_cols.index('speed_mps_prev2')\n",
        "\n",
        "print(f\"Feature columns count: {len(feature_cols)}\")\n",
        "print(f\"Updating feature indices: prev1={idx_prev1}, prev2={idx_prev2}\")\n",
        "\n",
        "# 3. Start Autoregressive Loop\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# Iterasi baris per baris\n",
        "for i in tqdm(range(len(df_test)), desc=\"Predicting per second\"):\n",
        "    \n",
        "    # A. LOGIC UPDATE FEATURE DARI PREDIKSI SEBELUMNYA\n",
        "    is_new_trip = False\n",
        "    if i == 0:\n",
        "        is_new_trip = True\n",
        "    elif 'trip_id' in df_test.columns and df_test.at[i, 'trip_id'] != df_test.at[i-1, 'trip_id']:\n",
        "        is_new_trip = True\n",
        "        \n",
        "    if not is_new_trip:\n",
        "        # Ambil hasil prediksi detik sebelumnya (i-1)\n",
        "        prev_pred_1 = predictions[i-1]\n",
        "        \n",
        "        # Ambil hasil prediksi 2 detik lalu (i-2), cek trip id yg sama\n",
        "        if i > 1 and df_test.at[i, 'trip_id'] == df_test.at[i-2, 'trip_id']:\n",
        "            prev_pred_2 = predictions[i-2]\n",
        "        else:\n",
        "            prev_pred_2 = prev_pred_1 # Fallback\n",
        "            \n",
        "        # --- UPDATE FITUR UTAMA DI DATAFRAME ---\n",
        "\n",
        "        df_test.at[i, 'speed_mps_prev1'] = prev_pred_1\n",
        "        df_test.at[i, 'speed_mps_prev2'] = prev_pred_2\n",
        "        \n",
        "    else:\n",
        "        #awal trip, kita asumsikan start dari 0 atau data asli t=0\n",
        "        df_test.at[i, 'speed_mps_prev1'] = 0.0\n",
        "        df_test.at[i, 'speed_mps_prev2'] = 0.0\n",
        "\n",
        "    # B. PREPARE INPUT VECTOR\n",
        "    X_row = df_test.loc[i, feature_cols].values.reshape(1, -1)\n",
        "    \n",
        "    # C. SCALE\n",
        "    X_row_scaled = scaler.transform(X_row)\n",
        "    \n",
        "    # D. PREDICT\n",
        "    pred_speed = speed_model.predict(X_row_scaled)[0]\n",
        "    \n",
        "    if pred_speed < 0: \n",
        "        pred_speed = 0.0\n",
        "        \n",
        "    predictions.append(pred_speed)\n",
        "\n",
        "# 4. Simpan ke DataFrame\n",
        "df_test['predicted_speed'] = predictions\n",
        "y_pred_speed = np.array(predictions)\n",
        "\n",
        "# 5. Hitung Akselerasi berdasarkan PREDIKSI SPEED (Speed Difference Method)\n",
        "# Accel = Pred_Speed(t) - Pred_Speed(t-1)\n",
        "# Hati-hati: t-1 nya harus dari trip yang sama.\n",
        "if 'trip_id' in df_test.columns:\n",
        "    df_test['pred_speed_prev'] = df_test.groupby('trip_id')['predicted_speed'].shift(1).fillna(0)\n",
        "    df_test['predicted_accel'] = df_test['predicted_speed'] - df_test['pred_speed_prev']\n",
        "else:\n",
        "    df_test['pred_speed_prev'] = df_test['predicted_speed'].shift(1).fillna(0)\n",
        "    df_test['predicted_accel'] = df_test['predicted_speed'] - df_test['pred_speed_prev']\n",
        "\n",
        "y_pred_accel = df_test['predicted_accel'].values\n",
        "y_test_speed = df_test['speed_mps'].values\n",
        "y_test_accel = df_test['acceleration'].values\n",
        "\n",
        "print(\"‚úÖ Autoregressive prediction complete.\")\n",
        "print(f\"   Speed predictions shape: {y_pred_speed.shape}\")\n",
        "print(f\"   Accel predictions shape: {y_pred_accel.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: Calculate Metrics\n",
        "print(\"\\n=== Calculating Quality Metrics ===\")\n",
        "\n",
        "# Speed Metrics\n",
        "speed_r2 = r2_score(y_test_speed, y_pred_speed)\n",
        "speed_rmse = np.sqrt(mean_squared_error(y_test_speed, y_pred_speed))\n",
        "speed_mae = mean_absolute_error(y_test_speed, y_pred_speed)\n",
        "speed_mse = mean_squared_error(y_test_speed, y_pred_speed)\n",
        "\n",
        "# Speed MAPE\n",
        "y_speed_safe = np.where(y_test_speed == 0, 1e-6, y_test_speed)\n",
        "speed_mape = np.mean(np.abs((y_test_speed - y_pred_speed) / y_speed_safe)) * 100\n",
        "\n",
        "# Acceleration Metrics\n",
        "accel_r2 = r2_score(y_test_accel, y_pred_accel)\n",
        "accel_rmse = np.sqrt(mean_squared_error(y_test_accel, y_pred_accel))\n",
        "accel_mae = mean_absolute_error(y_test_accel, y_pred_accel)\n",
        "accel_mse = mean_squared_error(y_test_accel, y_pred_accel)\n",
        "\n",
        "# Accel MAPE\n",
        "y_accel_safe = np.where(y_test_accel == 0, 1e-6, y_test_accel)\n",
        "accel_mape = np.mean(np.abs((y_test_accel - y_pred_accel) / y_accel_safe)) * 100\n",
        "\n",
        "# Statistical Comparison\n",
        "speed_mean_actual = np.mean(y_test_speed)\n",
        "speed_mean_pred = np.mean(y_pred_speed)\n",
        "speed_std_actual = np.std(y_test_speed)\n",
        "speed_std_pred = np.std(y_pred_speed)\n",
        "\n",
        "accel_mean_actual = np.mean(y_test_accel)\n",
        "accel_mean_pred = np.mean(y_pred_accel)\n",
        "accel_std_actual = np.std(y_test_accel)\n",
        "accel_std_pred = np.std(y_pred_accel)\n",
        "\n",
        "print(\"\\nüìä SPEED PREDICTION METRICS:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"R¬≤ Score:     {speed_r2:.4f}\")\n",
        "print(f\"RMSE:         {speed_rmse:.4f} m/s ({speed_rmse * 3.6:.2f} km/h)\")\n",
        "print(f\"MAE:          {speed_mae:.4f} m/s ({speed_mae * 3.6:.2f} km/h)\")\n",
        "print(f\"MSE:          {speed_mse:.4f}\")\n",
        "print(f\"MAPE:         {speed_mape:.2f}%\")\n",
        "print(f\"Mean Actual:  {speed_mean_actual:.4f} m/s\")\n",
        "print(f\"Mean Pred:    {speed_mean_pred:.4f} m/s\")\n",
        "print(f\"Std Actual:   {speed_std_actual:.4f} m/s\")\n",
        "print(f\"Std Pred:     {speed_std_pred:.4f} m/s\")\n",
        "\n",
        "print(\"\\nüìä ACCELERATION PREDICTION METRICS (from speed diff):\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"R¬≤ Score:     {accel_r2:.4f}\")\n",
        "print(f\"RMSE:         {accel_rmse:.4f} m/s¬≤\")\n",
        "print(f\"MAE:          {accel_mae:.4f} m/s¬≤\")\n",
        "print(f\"MSE:          {accel_mse:.4f}\")\n",
        "print(f\"MAPE:         {accel_mape:.2f}%\")\n",
        "print(f\"Mean Actual:  {accel_mean_actual:.4f} m/s¬≤\")\n",
        "print(f\"Mean Pred:    {accel_mean_pred:.4f} m/s¬≤\")\n",
        "print(f\"Std Actual:   {accel_std_actual:.4f} m/s¬≤\")\n",
        "print(f\"Std Pred:     {accel_std_pred:.4f} m/s¬≤\")\n",
        "\n",
        "# DEBUG: Detailed acceleration analysis\n",
        "print(f\"\\nüîç DEBUG: Why Is Acceleration Bad?\")\n",
        "print(f\"   Mean difference: {accel_mean_pred - accel_mean_actual:.4f} m/s¬≤ (should be ~0)\")\n",
        "print(f\"   Std ratio: {accel_std_pred / accel_std_actual:.4f} (should be ~1)\")\n",
        "print(f\"   Bias: {np.mean(y_pred_accel - y_test_accel):.4f} m/s¬≤\")\n",
        "\n",
        "# Check if speed predictions are smooth (causing accel issues)\n",
        "speed_changes = np.diff(y_pred_speed)\n",
        "actual_speed_changes = np.diff(y_test_speed)\n",
        "print(f\"\\n   Speed change analysis:\")\n",
        "print(f\"     Predicted speed std of changes: {np.std(speed_changes):.4f}\")\n",
        "print(f\"     Actual speed std of changes: {np.std(actual_speed_changes):.4f}\")\n",
        "print(f\"     Ratio: {np.std(speed_changes) / np.std(actual_speed_changes):.4f}\")\n",
        "\n",
        "if accel_r2 < 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  NEGATIVE R¬≤! This means:\")\n",
        "    print(f\"     - Model is worse than predicting mean\")\n",
        "    print(f\"     - Likely causes:\")\n",
        "    print(f\"       1. Speed predictions too smooth (low variance)\")\n",
        "    print(f\"       2. Model not capturing acceleration dynamics\")\n",
        "    print(f\"       3. Test set very different from training\")\n",
        "    print(f\"       4. Model predicting absolute speed, not changes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9: Create Visualization\n",
        "print(\"\\nGenerating validation plots...\")\n",
        "\n",
        "# Create two figure sets\n",
        "# Figure 1: Statistical Plots (6 subplots)\n",
        "fig1 = plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Speed - Actual vs Predicted Scatter\n",
        "plt.subplot(3, 3, 1)\n",
        "plt.scatter(y_test_speed, y_pred_speed, alpha=0.3, s=1)\n",
        "plt.plot([y_test_speed.min(), y_test_speed.max()], \n",
        "         [y_test_speed.min(), y_test_speed.max()], \n",
        "         'r--', linewidth=2, label='Perfect Prediction')\n",
        "plt.xlabel('Actual Speed (m/s)')\n",
        "plt.ylabel('Predicted Speed (m/s)')\n",
        "plt.title(f'Speed: Actual vs Predicted\\nR¬≤={speed_r2:.4f}, RMSE={speed_rmse:.4f} m/s')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Acceleration - Actual vs Predicted Scatter\n",
        "plt.subplot(3, 3, 2)\n",
        "plt.scatter(y_test_accel, y_pred_accel, alpha=0.3, s=1)\n",
        "plt.plot([y_test_accel.min(), y_test_accel.max()], \n",
        "         [y_test_accel.min(), y_test_accel.max()], \n",
        "         'r--', linewidth=2, label='Perfect Prediction')\n",
        "plt.xlabel('Actual Acceleration (m/s¬≤)')\n",
        "plt.ylabel('Predicted Acceleration (m/s¬≤)')\n",
        "plt.title(f'Accel: Actual vs Predicted (from speed diff)\\nR¬≤={accel_r2:.4f}, RMSE={accel_rmse:.4f} m/s¬≤')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Speed Error Distribution\n",
        "plt.subplot(3, 3, 3)\n",
        "speed_errors = y_test_speed - y_pred_speed\n",
        "plt.hist(speed_errors, bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Prediction Error (m/s)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(f'Speed Error Distribution\\nMAE={speed_mae:.4f} m/s')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Acceleration Error Distribution\n",
        "plt.subplot(3, 3, 4)\n",
        "accel_errors = y_test_accel - y_pred_accel\n",
        "plt.hist(accel_errors, bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Prediction Error (m/s¬≤)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(f'Acceleration Error Distribution\\nMAE={accel_mae:.4f} m/s¬≤')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Speed Statistics Comparison\n",
        "plt.subplot(3, 3, 5)\n",
        "categories = ['Mean', 'Std Dev']\n",
        "actual_vals = [speed_mean_actual, speed_std_actual]\n",
        "pred_vals = [speed_mean_pred, speed_std_pred]\n",
        "x = np.arange(len(categories))\n",
        "width = 0.35\n",
        "plt.bar(x - width/2, actual_vals, width, label='Actual', color='blue', alpha=0.7)\n",
        "plt.bar(x + width/2, pred_vals, width, label='Predicted', color='red', alpha=0.7)\n",
        "plt.ylabel('Speed (m/s)')\n",
        "plt.title('Speed Statistics Comparison')\n",
        "plt.xticks(x, categories)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Acceleration Statistics Comparison\n",
        "plt.subplot(3, 3, 6)\n",
        "actual_vals = [accel_mean_actual, accel_std_actual]\n",
        "pred_vals = [accel_mean_pred, accel_std_pred]\n",
        "plt.bar(x - width/2, actual_vals, width, label='Actual', color='blue', alpha=0.7)\n",
        "plt.bar(x + width/2, pred_vals, width, label='Predicted', color='red', alpha=0.7)\n",
        "plt.ylabel('Acceleration (m/s¬≤)')\n",
        "plt.title('Acceleration Statistics Comparison')\n",
        "plt.xticks(x, categories)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 7: Speed Time Series (Sample)\n",
        "plt.subplot(3, 3, 7)\n",
        "sample_size = min(500, len(y_test_speed))\n",
        "sample_idx = np.linspace(0, len(y_test_speed)-1, sample_size).astype(int)\n",
        "plt.plot(sample_idx, y_test_speed[sample_idx], 'b-', alpha=0.7, label='Actual', linewidth=1)\n",
        "plt.plot(sample_idx, y_pred_speed[sample_idx], 'r-', alpha=0.7, label='Predicted', linewidth=1)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Speed (m/s)')\n",
        "plt.title('Speed: Prediction vs Actual (Time Series)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 8: Acceleration Time Series (Sample)\n",
        "plt.subplot(3, 3, 8)\n",
        "plt.plot(sample_idx, y_test_accel[sample_idx], 'b-', alpha=0.7, label='Actual', linewidth=1)\n",
        "plt.plot(sample_idx, y_pred_accel[sample_idx], 'r-', alpha=0.7, label='Predicted', linewidth=1)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Acceleration (m/s¬≤)')\n",
        "plt.title('Accel: Prediction vs Actual (Time Series)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 9: Speed Residuals Over Time\n",
        "plt.subplot(3, 3, 9)\n",
        "plt.plot(sample_idx, speed_errors[sample_idx], 'g-', alpha=0.5, linewidth=0.8)\n",
        "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "plt.axhline(y=speed_mae, color='orange', linestyle=':', linewidth=1, label=f'MAE={speed_mae:.4f}')\n",
        "plt.axhline(y=-speed_mae, color='orange', linestyle=':', linewidth=1)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Error (m/s)')\n",
        "plt.title('Speed Prediction Errors Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "print(\"‚úÖ Statistical plots generated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 10: Evaluate Pass/Fail & Save Results\n",
        "print(\"\\n=== QUALITY EVALUATION ===\")\n",
        "\n",
        "# Compile metrics\n",
        "metrics = {\n",
        "    \"model_name\": model_name,\n",
        "    \"run_timestamp\": RUN_TIMESTAMP,\n",
        "    \"test_samples\": int(len(y_test_speed)),\n",
        "    \"acceleration_method\": \"speed_difference\",  # YOUR METHOD!\n",
        "    \"speed\": {\n",
        "        \"r2_score\": float(speed_r2),\n",
        "        \"rmse_ms\": float(speed_rmse),\n",
        "        \"rmse_kmh\": float(speed_rmse * 3.6),\n",
        "        \"mae_ms\": float(speed_mae),\n",
        "        \"mae_kmh\": float(speed_mae * 3.6),\n",
        "        \"mse\": float(speed_mse),\n",
        "        \"mape_percent\": float(speed_mape),\n",
        "        \"mean_actual\": float(speed_mean_actual),\n",
        "        \"mean_predicted\": float(speed_mean_pred),\n",
        "        \"std_actual\": float(speed_std_actual),\n",
        "        \"std_predicted\": float(speed_std_pred)\n",
        "    },\n",
        "    \"acceleration\": {\n",
        "        \"r2_score\": float(accel_r2),\n",
        "        \"rmse\": float(accel_rmse),\n",
        "        \"mae\": float(accel_mae),\n",
        "        \"mse\": float(accel_mse),\n",
        "        \"mape_percent\": float(accel_mape),\n",
        "        \"mean_actual\": float(accel_mean_actual),\n",
        "        \"mean_predicted\": float(accel_mean_pred),\n",
        "        \"std_actual\": float(accel_std_actual),\n",
        "        \"std_predicted\": float(accel_std_pred)\n",
        "    },\n",
        "    \"thresholds\": {\n",
        "        \"min_r2_score\": MIN_R2_SCORE,\n",
        "        \"max_speed_rmse_ms\": MAX_SPEED_RMSE,\n",
        "        \"max_accel_rmse\": MAX_ACCEL_RMSE,\n",
        "        \"max_speed_mae_ms\": MAX_SPEED_MAE,\n",
        "        \"max_accel_mae\": MAX_ACCEL_MAE\n",
        "    }\n",
        "}\n",
        "\n",
        "# Determine pass/fail\n",
        "failures = []\n",
        "if speed_r2 < MIN_R2_SCORE:\n",
        "    failures.append(f\"Speed R¬≤ {speed_r2:.4f} < {MIN_R2_SCORE}\")\n",
        "if speed_rmse > MAX_SPEED_RMSE:\n",
        "    failures.append(f\"Speed RMSE {speed_rmse:.4f} > {MAX_SPEED_RMSE} m/s\")\n",
        "if speed_mae > MAX_SPEED_MAE:\n",
        "    failures.append(f\"Speed MAE {speed_mae:.4f} > {MAX_SPEED_MAE} m/s\")\n",
        "if accel_rmse > MAX_ACCEL_RMSE:\n",
        "    failures.append(f\"Acceleration RMSE {accel_rmse:.4f} > {MAX_ACCEL_RMSE} m/s¬≤\")\n",
        "if accel_mae > MAX_ACCEL_MAE:\n",
        "    failures.append(f\"Acceleration MAE {accel_mae:.4f} > {MAX_ACCEL_MAE} m/s¬≤\")\n",
        "\n",
        "metrics[\"validation_passed\"] = len(failures) == 0\n",
        "metrics[\"failures\"] = failures\n",
        "\n",
        "# Print results\n",
        "print(json.dumps(metrics, indent=2))\n",
        "\n",
        "# Save plot\n",
        "img_buf = io.BytesIO()\n",
        "fig1.savefig(img_buf, format='png', dpi=150, bbox_inches='tight')\n",
        "img_buf.seek(0)\n",
        "with fs.open(OUTPUT_PLOT_PATH, 'wb') as f:\n",
        "    f.write(img_buf.getbuffer())\n",
        "print(f\"\\n‚úÖ Plots saved to {OUTPUT_PLOT_PATH}\")\n",
        "\n",
        "# Save metrics JSON\n",
        "with fs.open(OUTPUT_METRICS_PATH, 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print(f\"‚úÖ Metrics saved to {OUTPUT_METRICS_PATH}\")\n",
        "\n",
        "# Final verdict\n",
        "if failures:\n",
        "    print(f\"\\n‚ö†Ô∏è  Model Quality Validation Failed ({len(failures)} issues):\")\n",
        "    for failure in failures:\n",
        "        print(f\"  - {failure}\")\n",
        "    print(f\"\\nüí° Note: Thresholds have been relaxed for limited data testing\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Model Quality Validation Passed!\")\n",
        "    print(\"   All metrics within acceptable thresholds.\")\n",
        "\n",
        "print(\"\\n=== Validation Complete ===\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
